{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобраться с PyTorch-LifeStream сразу не получилось до конца. Да как водится времени не хватило"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.memory', '120g'),\n",
       " ('spark.driver.memoryOverhead', '4g'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.app.startTime', '1718545093063'),\n",
       " ('spark.driver.port', '37091'),\n",
       " ('spark.executor.memory', '120g'),\n",
       " ('spark.local.dir', '../../spark_local_dir'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.submitTime', '1718545092958'),\n",
       " ('spark.driver.host', '1fd1c056c77c'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.sql.shuffle.partitions', '200'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.sql.warehouse.dir', 'file:/content/Хакатон/spark-warehouse'),\n",
       " ('spark.driver.cores', '30'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1718545093564'),\n",
       " ('spark.cores.max', '24'),\n",
       " ('spark.app.name', 'PysparkDataPreprocessor'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.maxResultSize', '4g'),\n",
       " ('spark.executor.memoryOverhead', '4g')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "from ptls.preprocessing import PysparkDataPreprocessor\n",
    "\n",
    "\n",
    "data_path = 'data/'\n",
    "\n",
    "spark_conf = pyspark.SparkConf()\n",
    "spark_conf.setMaster(\"local[*]\").setAppName(\"PysparkDataPreprocessor\")\n",
    "spark_conf.set(\"spark.driver.cores\", 30) #Задаем только в Cluster Mode \n",
    "spark_conf.set(\"spark.driver.maxResultSize\", \"4g\")\n",
    "spark_conf.set(\"spark.executor.memory\", \"120g\")\n",
    "spark_conf.set(\"spark.executor.memoryOverhead\", \"4g\")\n",
    "spark_conf.set(\"spark.driver.memory\", \"120g\")\n",
    "spark_conf.set(\"spark.driver.memoryOverhead\", \"4g\")\n",
    "spark_conf.set(\"spark.cores.max\", \"24\")\n",
    "spark_conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark_conf.set(\"spark.local.dir\", \"../../spark_local_dir\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/colab/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/envs/colab/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from ptls.data_load.datasets import MemoryMapDataset\n",
    "from ptls.data_load.iterable_processing.iterable_seq_len_limit import ISeqLenLimit\n",
    "from ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\n",
    "from ptls.data_load.iterable_processing.feature_filter import FeatureFilter\n",
    "from ptls.nn import TrxEncoder, RnnSeqEncoder\n",
    "from ptls.frames.coles import CoLESModule\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter\n",
    "from ptls.frames.coles import ColesIterableDataset\n",
    "from ptls.frames.coles.split_strategy import SampleSlices\n",
    "from ptls.frames import PtlsDataModule\n",
    "from ptls.data_load.utils import collate_feature_dict\n",
    "from ptls.data_load.iterable_processing_dataset import IterableProcessingDataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import lightgbm as ltb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Открыть тестовые файлы с помощью spark\n",
    "geo_test = spark.read.parquet(\"data/geo_test.parquet\")\n",
    "# geo_train_sp = spark.read.parquet(\"data/geo_train.parquet\")\n",
    "trx_test = spark.read.parquet(\"data/trx_test.parquet\")\n",
    "# target_test = spark.read.parquet(\"data/test_target_b.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Открыть тренировочные файлы с помощью spark\n",
    "# geo_train = spark.read.parquet(\"data/geo_train.parquet\")\n",
    "trx_train = spark.read.parquet(\"data/trx_train.parquet\")\n",
    "# target_train = spark.read.parquet(\"data/train_target.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mon: string (nullable = true)\n",
      " |-- target_1: integer (nullable = true)\n",
      " |-- target_2: integer (nullable = true)\n",
      " |-- target_3: integer (nullable = true)\n",
      " |-- target_4: integer (nullable = true)\n",
      " |-- client_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mon: string (nullable = true)\n",
      " |-- target_1: integer (nullable = true)\n",
      " |-- target_2: integer (nullable = true)\n",
      " |-- target_3: integer (nullable = true)\n",
      " |-- target_4: integer (nullable = true)\n",
      " |-- client_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_test_20.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- amount: float (nullable = true)\n",
      " |-- client_id: string (nullable = true)\n",
      " |-- event_type: integer (nullable = true)\n",
      " |-- event_subtype: integer (nullable = true)\n",
      " |-- currency: integer (nullable = true)\n",
      " |-- src_type11: integer (nullable = true)\n",
      " |-- src_type12: integer (nullable = true)\n",
      " |-- dst_type11: integer (nullable = true)\n",
      " |-- dst_type12: integer (nullable = true)\n",
      " |-- src_type21: integer (nullable = true)\n",
      " |-- src_type22: integer (nullable = true)\n",
      " |-- src_type31: integer (nullable = true)\n",
      " |-- src_type32: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trx_test_20.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- client_id: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- geohash_4: integer (nullable = true)\n",
      " |-- geohash_5: integer (nullable = true)\n",
      " |-- geohash_6: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geo_test_20.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели для преобразования данных для ptls и трансформация данных через Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.preprocessing import PysparkDataPreprocessor\n",
    "\n",
    "trx_preprocessor = PysparkDataPreprocessor(\n",
    "    col_id='client_id',\n",
    "    col_event_time='event_time',\n",
    "    event_time_transformation='dt_to_timestamp',\n",
    "    cols_numerical=[\"event_type\",\n",
    "                   \"event_subtype\",\n",
    "                   \"currency\",\n",
    "                   \"src_type11\",\n",
    "                   \"src_type12\",\n",
    "                   \"dst_type11\",\n",
    "                   \"dst_type12\",\n",
    "                   \"src_type21\",\n",
    "                   \"src_type22\",\n",
    "                   \"src_type31\",\n",
    "                   \"src_type32\"],\n",
    "    cols_identity=\"amount\",\n",
    ")\n",
    "\n",
    "geo_preprocessor = PysparkDataPreprocessor(\n",
    "    col_id='client_id',\n",
    "    col_event_time='event_time',\n",
    "    event_time_transformation='dt_to_timestamp',\n",
    "    cols_numerical=['geohash_4',\n",
    "                    'geohash_5',\n",
    "                    'geohash_6']\n",
    ")\n",
    "\n",
    "target_preprocessor = PysparkDataPreprocessor(\n",
    "    col_id=\"client_id\",\n",
    "    col_event_time=\"mon\",\n",
    "    event_time_transformation=\"dt_to_timestamp\",\n",
    "    cols_identity=[\"target_1\", \"target_2\", \"target_3\", \"target_4\"]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.9 ms, sys: 2.75 ms, total: 19.7 ms\n",
      "Wall time: 158 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/16 10:41:37 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "proc_test = trx_preprocessor.fit_transform(trx_test).persist()\n",
    "proc_geo_test = geo_preprocessor.fit_transform(geo_test).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 ms, sys: 2.83 ms, total: 17.2 ms\n",
      "Wall time: 520 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/16 10:58:38 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "proc_train = trx_preprocessor.fit_transform(trx_train).persist()\n",
    "proc_geo_train = geo_preprocessor.fit_transform(geo_train).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***В виду того, что преобразование из формата spark в pandas очень затратный по времени, решил записать результат работы в parquet и потом прочитать в df***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Запись данных после препроцессинга в формат parquet\n",
    "proc_test.write.parquet('data/test.parquet', mode='overwrite')\n",
    "proc_geo_test.write.parquet('data/geo_test.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Запись данных после препроцессинга в формат parquet\n",
    "geo_train.write.parquet('data/proc_geo_train.parquet', mode='overwrite')\n",
    "proc_train.write.parquet('data/train.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/colab/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from functools import partial\n",
    "import pytorch_lightning as pl\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import datetime\n",
    "\n",
    "from ptls.data_load.datasets import MemoryMapDataset\n",
    "from ptls.data_load.iterable_processing.iterable_seq_len_limit import ISeqLenLimit\n",
    "from ptls.data_load.iterable_processing.to_torch_tensor import ToTorch\n",
    "from ptls.data_load.iterable_processing.feature_filter import FeatureFilter\n",
    "from ptls.nn import TrxEncoder, RnnSeqEncoder\n",
    "from ptls.frames.coles import CoLESModule\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter\n",
    "from ptls.frames.coles import ColesIterableDataset\n",
    "from ptls.frames.coles.split_strategy import SampleSlices\n",
    "from ptls.frames import PtlsDataModule\n",
    "from ptls.data_load.utils import collate_feature_dict\n",
    "from ptls.data_load.iterable_processing_dataset import IterableProcessingDataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import lightgbm as ltb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_parquet(\"data/preproc/test.parquet\")\n",
    "train = pd.read_parquet(\"data/preproc/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniq_values(val):\n",
    "    set_list = set()\n",
    "    for x in val:\n",
    "        if x is not None:\n",
    "            for y in list(x):\n",
    "                set_list.add(y)\n",
    "    return set_list\n",
    "\n",
    "columns_list = [\"event_type\",\"event_subtype\",\n",
    "                   \"currency\",\n",
    "                   \"src_type11\",\n",
    "                   \"src_type12\",\n",
    "                   \"dst_type11\",\n",
    "                   \"dst_type12\",\n",
    "                   \"src_type21\",\n",
    "                   \"src_type22\",\n",
    "                   \"src_type31\",\n",
    "                   \"src_type32\"\n",
    "                   ]\n",
    "\n",
    "len_dict = {}\n",
    "for x in columns_list:\n",
    "    y = str(x) + '_in'\n",
    "    len_dict[y] = len(uniq_values(test[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'event_type_in': 56,\n",
       " 'event_subtype_in': 58,\n",
       " 'currency_in': 55,\n",
       " 'src_type11_in': 266074,\n",
       " 'src_type12_in': 266282,\n",
       " 'dst_type11_in': 295587,\n",
       " 'dst_type12_in': 295829,\n",
       " 'src_type21_in': 50121,\n",
       " 'src_type22_in': 33841,\n",
       " 'src_type31_in': 62101,\n",
       " 'src_type32_in': 60246}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обработка датасета:**\n",
    "\n",
    "- Транзакции, у которых размер < min_seq_len выкидываются\n",
    "- Транзакции, у которых длина > max_seq_len, обрезаются и конвертируются в torch.tensor\n",
    "- Не нужные для CoLES фичи удаляются"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_m = MemoryMapDataset(\n",
    "    data=train.to_dict(\"records\"),\n",
    "    i_filters=[\n",
    "        FeatureFilter(drop_feature_names=['client_id', 'target_1', 'target_2', 'target_3', 'target_4']),\n",
    "        SeqLenFilter(min_seq_len=32),\n",
    "        ISeqLenLimit(max_seq_len=1024),\n",
    "        ToTorch()\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_m = MemoryMapDataset(\n",
    "    data=test.to_dict(\"records\"),\n",
    "    i_filters=[\n",
    "        FeatureFilter(drop_feature_names=['client_id', 'target_1', 'target_2', 'target_3', 'target_4']),\n",
    "        SeqLenFilter(min_seq_len=32),\n",
    "        ISeqLenLimit(max_seq_len=1024),\n",
    "        ToTorch()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ColesIterableDataset(\n",
    "    data=train_m,\n",
    "    splitter=SampleSlices(\n",
    "        split_count=5,\n",
    "        cnt_min=32,\n",
    "        cnt_max=90\n",
    "    )\n",
    ")\n",
    "\n",
    "valid_ds = ColesIterableDataset(\n",
    "    data=test_m,\n",
    "    splitter=SampleSlices(\n",
    "        split_count=5,\n",
    "        cnt_min=32,\n",
    "        cnt_max=90\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = PtlsDataModule(\n",
    "    train_data=train_ds,\n",
    "    train_num_workers=16,\n",
    "    train_batch_size=64,\n",
    "    valid_data=valid_ds,\n",
    "    valid_num_workers=16,\n",
    "    valid_batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "- numeric_values обрабатываются как BatchNorm+Linear\n",
    "- embedidngs - nn.Embedidngs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_encoder_params = dict(\n",
    "    embeddings_noise=0.003,\n",
    "    numeric_values={'amount': 'log'},\n",
    "    embeddings={\n",
    "        \"event_type\": {'in': len_dict['event_type_in'], \"out\": 12},\n",
    "        \"event_subtype\": {'in': len_dict[\"event_subtype_in\"], \"out\": 12},\n",
    "        'src_type11': {'in': len_dict[\"src_type11_in\"], 'out': 12},\n",
    "        'src_type12': {'in': len_dict[\"src_type12_in\"], 'out': 12},\n",
    "        'dst_type11': {'in': len_dict[\"dst_type11_in\"], 'out': 12},\n",
    "        'dst_type12': {'in': len_dict[\"dst_type12_in\"], 'out': 12},\n",
    "        'src_type22': {'in': len_dict[\"src_type22_in\"], 'out': 12},\n",
    "        'src_type31': {'in': len_dict[\"src_type31_in\"], 'out': 12},\n",
    "        'src_type32': {'in': len_dict[\"src_type32_in\"], 'out': 12},\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TrxEncoder** - обрабатывает каждую тразнакцию (строит для неё эмбеддиг)\n",
    "- **SeqEncoder** - обрабатывает последовательность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_encoder = RnnSeqEncoder(\n",
    "    trx_encoder=TrxEncoder(**trx_encoder_params),\n",
    "    hidden_size=32,\n",
    "    type='gru',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CoLESModule(\n",
    "    seq_encoder=seq_encoder,\n",
    "    optimizer_partial=partial(torch.optim.Adam, lr=0.001),\n",
    "    lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=3, gamma=0.9025)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    precision=\"16-mixed\",\n",
    "    max_epochs=2,\n",
    "    # limit_val_batches=000,\n",
    "    accelerator = \"gpu\",\n",
    "    devices = 1,\n",
    "    enable_progress_bar=True,\n",
    "    gradient_clip_val=0.5,\n",
    "    logger=pl.loggers.TensorBoardLogger(\n",
    "        save_dir='.data/logdir',\n",
    "        name='baseline_result'\n",
    "    ),\n",
    "    callbacks=[\n",
    "        pl.callbacks.LearningRateMonitor(logging_interval='step'),\n",
    "        pl.callbacks.ModelCheckpoint(every_n_train_steps=2000, save_top_k=-1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = pl.Trainer(\n",
    "#     precision=\"16-mixed\",\n",
    "#     max_epochs=2,\n",
    "#     accelerator = \"gpu\",\n",
    "#     devices = 1,\n",
    "#     enable_progress_bar=True,\n",
    "#     gradient_clip_val=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type            | Params\n",
      "-------------------------------------------------------\n",
      "0 | _loss              | ContrastiveLoss | 0     \n",
      "1 | _seq_encoder       | RnnSeqEncoder   | 15.4 M\n",
      "2 | _validation_metric | BatchRecallTopK | 0     \n",
      "3 | _head              | Head            | 0     \n",
      "-------------------------------------------------------\n",
      "15.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "15.4 M    Total params\n",
      "61.499    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03dc0c4c32764ca2b7c79a2671064980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=2000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'data/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetSplit(IterableProcessingDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_month,\n",
    "        end_month,\n",
    "        year=2022,\n",
    "        col_id='client_id',\n",
    "        col_time='event_time'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.start_month = start_month\n",
    "        self.end_month = end_month\n",
    "        self._year = year\n",
    "        self._col_id = col_id\n",
    "        self._col_time = col_time\n",
    "\n",
    "    def __iter__(self):\n",
    "        for rec in self._src:\n",
    "            for month in range(self.start_month, self.end_month+1):\n",
    "                features = rec[0] if type(rec) is tuple else rec\n",
    "                features = features.copy()\n",
    "\n",
    "                if month == 12:\n",
    "                    month_event_time = datetime(self._year + 1, 1, 1).timestamp()\n",
    "                else:\n",
    "                    month_event_time = datetime(self._year, month + 1, 1).timestamp()\n",
    "\n",
    "                year_event_time = datetime(self._year, 1, 1).timestamp()\n",
    "\n",
    "                mask = features[self._col_time] < month_event_time\n",
    "\n",
    "                for key, tensor in features.items():\n",
    "                    if key.startswith('target'):\n",
    "                        features[key] = tensor[month - 1].tolist()\n",
    "                    elif key != self._col_id:\n",
    "                        features[key] = tensor[mask]\n",
    "\n",
    "                features[self._col_id] += '_month=' + str(month)\n",
    "\n",
    "                yield features\n",
    "\n",
    "def collate_feature_dict_with_target(batch, col_id='client_id', targets=False):\n",
    "    batch_ids = []\n",
    "    target_cols = []\n",
    "    for sample in batch:\n",
    "        batch_ids.append(sample[col_id])\n",
    "        del sample[col_id]\n",
    "\n",
    "        if targets:\n",
    "            target_cols.append([sample[f'target_{i}'] for i in range(1, 5)])\n",
    "            del sample['target_1']\n",
    "            del sample['target_2']\n",
    "            del sample['target_3']\n",
    "            del sample['target_4']\n",
    "\n",
    "    padded_batch = collate_feature_dict(batch)\n",
    "    if targets:\n",
    "        return padded_batch, batch_ids, target_cols\n",
    "    return padded_batch, batch_ids\n",
    "\n",
    "\n",
    "class InferenceModuleMultimodal(pl.LightningModule):\n",
    "    def __init__(self, model, pandas_output=True, drop_seq_features=True, model_out_name='out'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.pandas_output = pandas_output\n",
    "        self.drop_seq_features = drop_seq_features\n",
    "        self.model_out_name = model_out_name\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_len = len(x)\n",
    "        if x_len == 3:\n",
    "            x, batch_ids, target_cols = x\n",
    "        else:\n",
    "            x, batch_ids = x\n",
    "\n",
    "        out = self.model(x)\n",
    "        if x_len == 3:\n",
    "            target_cols = torch.tensor(target_cols)\n",
    "            x_out = {\n",
    "                'client_id': batch_ids,\n",
    "                'target_1': target_cols[:, 0],\n",
    "                'target_2': target_cols[:, 1],\n",
    "                'target_3': target_cols[:, 2],\n",
    "                'target_4': target_cols[:, 3],\n",
    "                self.model_out_name: out\n",
    "            }\n",
    "        else:\n",
    "            x_out = {\n",
    "                'client_id': batch_ids,\n",
    "                self.model_out_name: out\n",
    "            }\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if self.pandas_output:\n",
    "            return self.to_pandas(x_out)\n",
    "        return x_out\n",
    "\n",
    "    @staticmethod\n",
    "    def to_pandas(x):\n",
    "        expand_cols = []\n",
    "        scalar_features = {}\n",
    "\n",
    "        for k, v in x.items():\n",
    "            if type(v) is torch.Tensor:\n",
    "                v = v.cpu().numpy()\n",
    "\n",
    "            if type(v) is list or len(v.shape) == 1:\n",
    "                scalar_features[k] = v\n",
    "            elif len(v.shape) == 2:\n",
    "                expand_cols.append(k)\n",
    "            else:\n",
    "                scalar_features[k] = None\n",
    "\n",
    "        dataframes = [pd.DataFrame(scalar_features)]\n",
    "        for col in expand_cols:\n",
    "            v = x[col].cpu().numpy()\n",
    "            dataframes.append(pd.DataFrame(v, columns=[f'{col}_{i:04d}' for i in range(v.shape[1])]))\n",
    "\n",
    "        return pd.concat(dataframes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.preprocessing import PandasDataPreprocessor\n",
    "target_train = pd.read_parquet(\"data/train_target.parquet\")\n",
    "\n",
    "target_preprocessor = PandasDataPreprocessor(\n",
    "    col_id=\"client_id\",\n",
    "    col_event_time=\"mon\",\n",
    "    event_time_transformation=\"dt_to_timestamp\",\n",
    "    cols_identity=[\"target_1\", \"target_2\", \"target_3\", \"target_4\"],\n",
    "    # return_records=False,\n",
    ")\n",
    "\n",
    "processed_target = target_preprocessor.fit_transform(target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in processed_target:\n",
    "    del i[\"event_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'client_id': '000006265d27d1166ed67506682be7380007a5bead4362f0a9795f7d97fb08e3',\n",
       "  'target_1': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       "  'target_2': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32),\n",
       "  'target_3': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], dtype=torch.int32),\n",
       "  'target_4': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_target[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pd.DataFrame(processed_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/envs/colab/lib/python3.10/site-packages/ptls/data_load/datasets/memory_dataset.py:18\u001b[0m, in \u001b[0;36mMemoryMapDataset.__init__\u001b[0;34m(self, data, i_filters)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     post_processor_filter \u001b[38;5;241m=\u001b[39m IterableChain(\u001b[38;5;241m*\u001b[39mi_filters)\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_data \u001b[38;5;241m=\u001b[39m [rec \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m post_processor_filter(data)]\n\u001b[1;32m     19\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/colab/lib/python3.10/site-packages/ptls/data_load/datasets/memory_dataset.py:18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     post_processor_filter \u001b[38;5;241m=\u001b[39m IterableChain(\u001b[38;5;241m*\u001b[39mi_filters)\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_data \u001b[38;5;241m=\u001b[39m [rec \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m post_processor_filter(data)]\n\u001b[1;32m     19\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/colab/lib/python3.10/site-packages/ptls/data_load/iterable_processing/to_torch_tensor.py:17\u001b[0m, in \u001b[0;36mToTorch.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rec \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_src:\n\u001b[1;32m     18\u001b[0m         features \u001b[38;5;241m=\u001b[39m rec[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(rec) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m rec\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m, in \u001b[0;36mGetSplit.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m     month_event_time \u001b[38;5;241m=\u001b[39m datetime(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_year \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtimestamp()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     month_event_time \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_year\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtimestamp()\n\u001b[1;32m     28\u001b[0m year_event_time \u001b[38;5;241m=\u001b[39m datetime(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_year, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtimestamp()\n\u001b[1;32m     30\u001b[0m mask \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_col_time] \u001b[38;5;241m<\u001b[39m month_event_time\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train = MemoryMapDataset(\n",
    "    data=train.merge(target_df, on=\"client_id\", how=\"inner\").to_dict(\"records\"),\n",
    "    i_filters=[\n",
    "        ISeqLenLimit(max_seq_len=4096),\n",
    "        FeatureFilter(keep_feature_names=['client_id', 'target_1', 'target_2', 'target_3', 'target_4']),\n",
    "        GetSplit(start_month=1, end_month=12),\n",
    "        ToTorch(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test = MemoryMapDataset(\n",
    "    data=test.to_dict(\"records\"),\n",
    "    i_filters=[\n",
    "        ISeqLenLimit(max_seq_len=4096),\n",
    "        FeatureFilter(keep_feature_names=['client_id', 'target_1', 'target_2', 'target_3', 'target_4']),\n",
    "        ToTorch(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наверное надо было получить эмбединги и обучать ml модели по классике"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
